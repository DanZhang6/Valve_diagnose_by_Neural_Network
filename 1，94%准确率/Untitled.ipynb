{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils1 import load_dataset1, random_mini_batches, convert_to_one_hot, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Train_Y, Dev_X, Dev_Y, Test_X, Test_Y, X_Anomaly=load_dataset1();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### ( approx. 4 lines of code)\n",
    "    # Create a placeholder for x. Name it 'x'.\n",
    "    x = tf.placeholder(tf.float32, name=\"x\")\n",
    "\n",
    "    # compute sigmoid(x)\n",
    "    sigmoid = tf.sigmoid(x)\n",
    "\n",
    "    # Create a session, and run it. Please use the method 2 explained above. \n",
    "    # You should use a feed_dict to pass z's value to x. \n",
    "    with tf.Session() as sess: \n",
    "    # run the variables initialization (if needed), run the operations\n",
    "        # Run session and call the output \"result\"\n",
    "        result = sess.run(sigmoid, feed_dict = {x: z})\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    #C = tf.constant(C, name = \"C\")\n",
    "    depth = tf.constant(value = C, name = 'C')\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    #one_hot_matrix = tf.one_hot(labels, C, axis = 0)\n",
    "    one_hot_matrix = tf.one_hot(labels, depth, axis = 0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 16880\n",
      "number of test examples = 5629\n",
      "X_train shape: (280, 16880)\n",
      "Y_train shape: (5, 16880, 1)\n",
      "X_test shape: (280, 5629)\n",
      "Y_test shape: (5, 5629, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_train_orig=np.matrix(Train_Y[1,:])\n",
    "Y_test_orig=np.matrix(Dev_Y[1,:])\n",
    "X_train=Train_X\n",
    "X_test=Dev_X\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "Y_train=np.delete(Y_train, 0, 0)\n",
    "Y_test=np.delete(Y_test, 0, 0)\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    #X = tf.placeholder(tf.float32, shape = (n_x, None), name = \"X\")\n",
    "    #Y = tf.placeholder(tf.float32, shape = (n_y, None), name = \"Y\")\n",
    "    X = tf.placeholder(tf.float32, shape=[n_x, None])\n",
    "    Y = tf.placeholder(tf.float32, shape=[n_y, None])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [25,280], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [5,12], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable(\"b3\", [5,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)                        # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                       # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)                       # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                       # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)                       # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    #cost = tf.nn.softmax_cross_entropy_with_logits (labels = labels, logits = logits)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    #tf.set_random_seed(1)                             # to keep consistent results\n",
    "    #seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.129708\n",
      "Cost after epoch 100: 0.389714\n",
      "Cost after epoch 200: 0.259280\n",
      "Cost after epoch 300: 0.223740\n",
      "Cost after epoch 400: 0.202181\n",
      "Cost after epoch 500: 0.182162\n",
      "Cost after epoch 600: 0.169170\n",
      "Cost after epoch 700: 0.165494\n",
      "Cost after epoch 800: 0.148287\n",
      "Cost after epoch 900: 0.137448\n",
      "Cost after epoch 1000: 0.134395\n",
      "Cost after epoch 1100: 0.122314\n",
      "Cost after epoch 1200: 0.118522\n",
      "Cost after epoch 1300: 0.118081\n",
      "Cost after epoch 1400: 0.108367\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8JHWd//HXp9O573uOZO57huEaGVBRBEVAFA9cb9Td\nFXFl3Z/Hb9E9PNafux7LrrrK4o33jYKIgKgIghzDOPd9z2SSSTKZ3Hfy+f1RldCEXAPpdHr6/Xw8\n+pHuquqqzzc10+9Ufb9Vbe6OiIgIQCTRBYiIyMyhUBARkWEKBRERGaZQEBGRYQoFEREZplAQEZFh\nCgU5I5jZb8zs7YmuQyTZKRTkOTGzQ2b20kTX4e5Xuvu3E10HgJk9YGZ/Ow3byTSzb5pZq5nVmdkH\nJlj+zWZ22Mw6zOyXZlYy2XWZ2Tlm9qSZdYY/z4mZt8bM7jWzRjPThU9JTqEgM56ZRRNdw5CZVAvw\ncWApMB94CfCPZnbFaAua2WrgK8DbgEqgE7hlMusyswzgDuB7QDHwbeCOcDpAH/AT4G+mrmmSMO6u\nhx7P+gEcAl46xryrgU1AM/AIsDZm3oeB/UAbsAN4Tcy8dwAPA/8NnAT+XzjtT8B/AqeAg8CVMe95\nAPjbmPePt+xC4MFw2/cDXwa+N0YbLgGOATcBdcB3CT4Y7wIawvXfBVSFy38KGAC6gXbgS+H0FcBv\ngSZgN/BXU/C7Pw5cHvP634AfjbHsvwM/iHm9GOgF8idaF3A5UANYzPwjwBUjtrEk+EhJ/L9LPZ79\nQ0cKEhdmdi7wTeDdQCnBX6l3mllmuMh+4GKgEPgE8D0zmx2zivXAAYK/aj8VM203UAZ8FviGmdkY\nJYy37A+Ax8O6Pk7w1/N4ZgElBH9FX09whP2t8PU8oAv4EoC7/zPwEHCju+e5+41mlksQCD8AKoA3\nAreY2arRNmZmt5hZ8xiPLeEyxcBsYHPMWzcDq8dow+rYZd19P9ADLJvEulYDWzz85J/EtiSJKRQk\nXq4HvuLuj7n7gAfn+3uACwHc/afuftzdB939x8Be4IKY9x939/9x93537wqnHXb3r7n7AMEpjNkE\noTGaUZc1s3nA84CPunuvu/8JuHOCtgwCH3P3HnfvcveT7v5zd+909zaC0HrxOO+/Gjjk7t8K2/MX\n4OfA60db2N3/zt2LxnisDRfLC3+2xLy1Fcgfo4a8EcvGLj/RusZ7r5xhFAoSL/OBD8b+lQtUA3MA\nzOw6M9sUM28NwV/1Q46Oss66oSfu3hk+zRtlufGWnQM0xUwba1uxGty9e+iFmeWY2VfCTttWglNR\nRWaWNsb75wPrR/wu3kJwBPJstYc/C2KmFRKcEhtr+YIR04aWn2hd471XzjAKBYmXo8CnRvyVm+Pu\nPzSz+cDXgBuBUncvArYBsaeC4jWKpRYoMbOcmGnVE7xnZC0fBJYD6929AHhRON3GWP4o8McRv4s8\nd3/PaBszs1vNrH2Mx3YAdz8VtuXsmLeeDWwfow3bY5c1s8VABrBnEuvaDqwdcapu7TjbkiSmUJCp\nkG5mWTGPKMGH/g1mtt4CuWb2CjPLB3IJPjgbAMzsnQRHCnHn7oeBDcDHzSzDzC4CXnmaq8kn6Edo\nDod1fmzE/BPAopjXdxGcu3+bmaWHj+eZ2coxarwhDI3RHrHn8b8D/IuZFYfrehdw2xg1fx94pZld\nHPZxfBK4PTz9NdG6HiDoPH9fOHT1fQT77/cA4f7NIggZwn8DQ31HkmQUCjIV7ib4kBx6fNzdNxB8\nsHyJYITOPoJRQbj7DuBm4M8EH6BnEYw2mi5vAS7iqZFNPybo75iszwPZQCPwKHDPiPlfAK41s1Nm\n9sXwg/dygg7m4wSntj4DPNcPzo8RdNgfJvjg/qy7D9cSHllcDODu24EbCMKhniCY/24y63L3XuDV\nwHUEI8neAbw6nA7B6bEunjpy6CLo5JckZE8fUCCSeszsx8Audx/5F79IytGRgqSc8NTNYjOLhBdo\nXQP8MtF1icwEM+nqTJHpMgu4neA6hWPAe8JhoiIpT6ePRERkmE4fiYjIsKQ7fVRWVuYLFixIdBki\nIknlySefbHT38omWS7pQWLBgARs2bEh0GSIiScXMDk9mOZ0+EhGRYQoFEREZplAQEZFhCgURERmm\nUBARkWEKBRERGaZQEBGRYSkTCrvr2rj5vt2cbD+dOySLiKSWlAmF/Q3t/M/v99GgUBARGVPKhEJG\nWtDUnr7BBFciIjJzpUwoZKYHTe0dUCiIiIwlZUJh6Eiht1+hICIyltQJhahCQURkIikXCj0KBRGR\nMaVMKGRG0wDo6R9IcCUiIjNXCoWCTh+JiEwkZUJhuE9Bo49ERMaUOqGg0UciIhNKmVAYuk5BHc0i\nImNLmVDQkYKIyMRSJhSiaREiplAQERlPyoQCBJ3N6mgWERlbSoVCZjRNRwoiIuNIqVDIiEZ08ZqI\nyDhSKxTSIhp9JCIyjpQKhcxoRKePRETGkVKhkKFQEBEZV0qFQmZUp49ERMaTUqGgIwURkfGlXijo\nOgURkTHFLRTM7JtmVm9m28aYb2b2RTPbZ2ZbzOy8eNUyRNcpiIiML55HCrcBV4wz/0pgafi4Hvjf\nONYCBENSFQoiImOLWyi4+4NA0ziLXAN8xwOPAkVmNjte9YAuXhMRmUgi+xTmAkdjXh8Lpz2DmV1v\nZhvMbENDQ8Oz3qA6mkVExpcUHc3u/lV3X+fu68rLy5/1etTRLCIyvkSGQg1QHfO6KpwWN7pOQURk\nfIkMhTuB68JRSBcCLe5eG88NZigURETGFY3Xis3sh8AlQJmZHQM+BqQDuPutwN3AVcA+oBN4Z7xq\nGZIZjj5yd8ws3psTEUk6cQsFd3/TBPMdeG+8tj+ajGhwYNQ34GREFQoiIiMlRUfzVMmMpgGos1lE\nZAwpFQpDRwoalioiMrqUDAVdwCYiMrrUCoU0HSmIiIwnpUIhM12hICIynpQKhYKsdABOdvQmuBIR\nkZkppUJhWWU+AHtOtCW4EhGRmSmlQqGyIJPC7HR21SkURERGk1KhYGYsn5XPboWCiMioUioUAFbM\nymdPXRvBBdUiIhIr5UJhWWU+bT391DR3JboUEZEZJ+VCYcUsdTaLiIwl5UJhWRgK6mwWEXmmlAuF\ngqx05hZlq7NZRGQUKRcKgEYgiYiMIWVDYX9DO326hbaIyNOkZihU5tM34Bxo6Eh0KSIiM0pKhsKS\nijwADja2J7gSEZGZJSVDYX5pDgCHTnYmuBIRkZklJUMhPyudsrwMDjXq9JGISKyUDAWA+aW5HDqp\nUBARiZWyobCgNJdDjTp9JCISK4VDIYe61m66evV9zSIiQ1I3FMpyATjcpFNIIiJDUjcUSoNQ0Ckk\nEZGnpGwozC8bGpaqIwURkSEpGwoFWemU5mZwWKEgIjIsZUMBgn6Fg7pWQURkWEqHwvzSHA7rqmYR\nkWEpHQoLS3OpbdGwVBGRIXENBTO7wsx2m9k+M/vwKPMLzexXZrbZzLab2TvjWc9I88NhqUeadLQg\nIgJxDAUzSwO+DFwJrALeZGarRiz2XmCHu58NXALcbGYZ8apppAXhjfHU2SwiEojnkcIFwD53P+Du\nvcCPgGtGLONAvpkZkAc0Af1xrOlpqouDUDh6qmu6NikiMqPFMxTmAkdjXh8Lp8X6ErASOA5sBf7B\n3Z/xdWhmdr2ZbTCzDQ0NDVNWYFFOOnmZUY7q9JGICJD4juaXA5uAOcA5wJfMrGDkQu7+VXdf5+7r\nysvLp2zjZkZVcbZCQUQkFM9QqAGqY15XhdNivRO43QP7gIPAijjW9AzVJTkcPaVQEBGB+IbCE8BS\nM1sYdh6/EbhzxDJHgMsAzKwSWA4ciGNNz1BdnMPRpi7cfTo3KyIyI0XjtWJ37zezG4F7gTTgm+6+\n3cxuCOffCnwSuM3MtgIG3OTujfGqaTTzSrLp6hvgZEcvZXmZ07lpEZEZJ26hAODudwN3j5h2a8zz\n48Dl8axhItUl4Qikpk6FgoikvER3NCfcUCjoAjYREYUCVcXZABzTtQoiIgqFnIwoZXkZGpYqIoJC\nAYCqYg1LFREBhQIQXqvQpNNHIiIKBYJhqcebuxgY1LUKIpLaFAoEF7D1Dzq1LTpaEJHUplBAw1JF\nRIYoFAi+lhPQV3OKSMpTKABzCrPJjEY40NCe6FJERBJKoQBEIsbCslwONuob2EQktSkUQgvLcjnQ\noFAQkdSmUAgtKs/lSFMnfQPP+OI3EZGUoVAILSzLo3/QdQ8kEUlpCoXQovJcAHU2i0hKUyiEFpUF\noaDOZhFJZQqFUFFOBsU56exXZ7OIpDCFQoxF5XkcbNTpIxFJXQqFGLpWQURSnUIhxqLyXE609tDe\n05/oUkREEkKhEGOos/mQjhZEJEVNKhTM7PWTmZbsFpblAbBfw1JFJEVN9kjhI5OcltQWlOWQFjH2\n1SsURCQ1RcebaWZXAlcBc83sizGzCoAz7sR7ZjSN+aU57D2hUBCR1DRuKADHgQ3Aq4AnY6a3Ae+P\nV1GJtLQij731bYkuQ0QkIcYNBXffDGw2sx+4ex+AmRUD1e5+ajoKnG5LK/K5f2c9vf2DZETVDy8i\nqWWyn3q/NbMCMysBNgJfM7P/jmNdCbO0Mo+BQefQSY1AEpHUM9lQKHT3VuC1wHfcfT1wWfzKSpwl\nFcEIpN11OoUkIqlnsqEQNbPZwF8Bd8WxnoRbUpFHNGLsqG1NdCkiItNusqHwb8C9wH53f8LMFgF7\nJ3qTmV1hZrvNbJ+ZfXiMZS4xs01mtt3M/jj50uMjM5rG0sp8th9XKIhI6plo9BEA7v5T4Kcxrw8A\nrxvvPWaWBnwZeBlwDHjCzO509x0xyxQBtwBXuPsRM6s4/SZMvdVzCnhgdz3ujpkluhwRkWkz2Sua\nq8zsF2ZWHz5+bmZVE7ztAmCfux9w917gR8A1I5Z5M3C7ux8BcPf6021APKyeU0Bjey/1bT2JLkVE\nZFpN9vTRt4A7gTnh41fhtPHMBY7GvD4WTou1DCg2swfM7Ekzu260FZnZ9Wa2wcw2NDQ0TLLkZ2/1\nnEIAth9vifu2RERmksmGQrm7f8vd+8PHbUD5FGw/CpwPvAJ4OfCvZrZs5ELu/lV3X+fu68rLp2Kz\n41s5Ox+AHepXEJEUM9lQOGlmbzWztPDxVuDkBO+pAapjXleF02IdA+519w53bwQeBM6eZE1xk5+V\nzoLSHHU2i0jKmWwo/DXBcNQ6oBa4FnjHBO95AlhqZgvNLAN4I8EpqFh3AC80s6iZ5QDrgZ2TrCmu\nVs0pUCiISMo5nSGpb3f3cnevIAiJT4z3BnfvB24kGMq6E/iJu283sxvM7IZwmZ3APcAW4HHg6+6+\n7dk1ZWqtnlPIkaZOWrv7El2KiMi0mdSQVGBt7L2O3L3JzM6d6E3ufjdw94hpt454/Tngc5OsY9qs\nmlMABP0KFy4qTXA1IiLTY7JHCpHwRngAhPdAmmygJKXVYSjoFJKIpJLJfrDfDPzZzIYuYHs98Kn4\nlDQzVORnUZ6fqWGpIpJSJntF83fMbANwaTjptbFXJp+pVs8p0LBUEUkpkz4FFIbAGR8EsVbPKeCh\nvY109w2QlZ6W6HJEROJO3yIzjtVzChkYdPac0G20RSQ1KBTGoc5mEUk1CoVxVBfnkJcZVb+CiKQM\nhcI4IhFj1ewCjUASkZShUJjAqjkF7KxtY2DQE12KiEjcKRQmsHpOAV19Axxs7Eh0KSIicadQmIC+\nW0FEUolCYQJLK/PIiEY0AklEUoJCYQLpaRFWzspny7HmRJciIhJ3CoVJOKuqkO01rQyqs1lEznAK\nhUk4a24hbT39HG7qTHQpIiJxpVCYhDVzg87mrTXqbBaRM5tCYRKWVeaTk5HGxsOnJl5YRCSJKRQm\nIT0twvnzi3n0wMlElyIiElcKhUlav7CEXXVtnOroTXQpIiJxo1CYpPXh9zQ/fqgpwZWIiMSPQmGS\n1lYVkhmN8NgBhYKInLkUCpOUGU3j/PnFPHZQ/QoicuZSKJyG9QtL2VHbSktXX6JLERGJC4XCaVi/\nqAR32KB+BRE5QykUTsM51UVkRCMamioiZyyFwmnISk/jnOoiHjuoIwUROTMpFE7ThQtL2FbTQlu3\n+hVE5MyjUDhN6xeVMuiwQbe8EJEzkELhNJ03r5iMtAgP721MdCkiIlMurqFgZleY2W4z22dmHx5n\nueeZWb+ZXRvPeqZCdkYaFy4u5Xe76hNdiojIlItbKJhZGvBl4EpgFfAmM1s1xnKfAe6LVy1T7WUr\nKzjY2MH+hvZElyIiMqXieaRwAbDP3Q+4ey/wI+CaUZb7e+DnQNL86X3pykoA7tt+IsGViIhMrXiG\nwlzgaMzrY+G0YWY2F3gN8L9xrGPKzS3K5pzqIu7YVJPoUkREplSiO5o/D9zk7oPjLWRm15vZBjPb\n0NDQME2lje81585lV10bO2tbE12KiMiUiWco1ADVMa+rwmmx1gE/MrNDwLXALWb26pErcvevuvs6\nd19XXl4er3pPy9VrZxONGHdsOp7oUkREpkw8Q+EJYKmZLTSzDOCNwJ2xC7j7Qndf4O4LgJ8Bf+fu\nv4xjTVOmNC+TixaXcu/2Otw90eWIiEyJuIWCu/cDNwL3AjuBn7j7djO7wcxuiNd2p9PLV8/iYGMH\ne+s1CklEzgzReK7c3e8G7h4x7dYxln1HPGuJh8tXVfKvd2zj7q21LKvMT3Q5IiLPWaI7mpNaRUEW\nL1hcxk83HGNgUKeQRCT5KRSeo7esn0dNcxcP7E6ayyxERMakUHiOXrqqktmFWXzmnl109w0kuhwR\nkedEofAcpadF+I/XnsWeE+3c8sD+RJcjIvKcKBSmwCXLK7jqrFl86+GD+p4FEUlqCoUp8p4XL6Gt\nu5/v/PlwoksREXnWFApT5KyqQi5bUcH/PrCf+tbuRJcjIvKsKBSm0L9evYre/kE+esd2XeUsIklJ\noTCFFpTl8sHLl3HP9jq+96hOI4lI8lEoTLF3XbyIS5aX88m7drL9eEuiyxEROS0KhSkWiRg3v/5s\ninPTufEHf6G9pz/RJYmITJpCIQ5K8zL5whvP5fDJDm762RYGdQsMEUkSCoU4uXBRKTddsYJfb63l\nP36zUx3PIpIU4nqX1FR3/YsWUdPcxdceOkhP/yCfeNVqzCzRZYmIjEmhEEdmxidetZrMaISvPXSQ\n8rxM/v6ypYkuS0RkTAqFODMzPnLlShrbe7n5t3voH3Te/7JliS5LRGRUCoVpMDQiyQy++Pu9vHBp\nGc9bUJLoskREnkEdzdMkEjE+ec0aqoqzue4bj3PLA/v0xTwiMuMoFKZRbmaUH/zthbxoWRmfvWc3\nb/vGY7R06q6qIjJzKBSmWXVJDre+9Xw+e+1anjjUxEtufoBP/0Zf0CMiM4NCIQHMjL9aV81P3n0R\nFy4q4dY/7ucVX3yIP+1tpKdf4SAiiaNQSKBz5xVzy1vO57t/cwEdPQO89RuP8ZLPPcADu+t1WklE\nEkKhMANcvLSc+z/4Yr785vMAeMe3nuDF//kHttW0KBxEZFpZst1+Yd26db5hw4ZElxE3LZ19PLK/\nkY/8YivNnX1kRCPcdMUKzqku4vz5xYkuT0SSlJk96e7rJlpO1ynMMIU56Vx51mwqCrL4w656Htnf\nyCfv2gHA37xwIVedNYvz5hXrdhkiEhc6Upjh+gcGOdDYwdcfOsBPNhwDoLokm4sWlfL25y9g9ZzC\nBFcoIslgskcKCoUk0tDWw4N7GvjNtjoePXCSjt5+rn/RIs6bV8zuujZWzMqnrrWb151XRW6mDgJF\n5CkKhTNcS1cfn7xrBz978tgz5i2vzOfy1ZUsLs9jcXkei8pzFRIiKU6hkCKONnVypKmTJRV57K5r\no7N3gM/du4uDjR0M3UUjGjE+cPky3nXxItLTNOBMJBUpFFJcT/8AR052sr+hgzs21fCbbXUU56Sz\nrDKf+rYeLl1RQWY0wtLKPK5cM5us9LRElywicTQjQsHMrgC+AKQBX3f3T4+Y/xbgJsCANuA97r55\nvHUqFE6fu/OH3fXctaWWg40d5GZEeXh/YzgP5pXk0NHTz+yiLD7wsmW4w3nziinOzUhw5SIyVRIe\nCmaWBuwBXgYcA54A3uTuO2KWeT6w091PmdmVwMfdff1461UoTI2Wrj5yMtJ4aG8Dn79/L9UlOeyq\nbWV/QwcASyvyyMlIY05RNh+8fBlLKvKpae6ir3+QeSU5RCIaEiuSTGbCdQoXAPvc/UBY0I+Aa4Dh\nUHD3R2KWfxSoimM9EqMwOx2AS1dUcumKSgA6evr5n9/vIz8ryufv38OswiwONnbwxz0NnFNdxJ8P\nnMQdFpTmcFZVEedUF/HmC+aRnhYERFT9FSJJL55HCtcCV7j734av3wasd/cbx1j+Q8CKoeVHzLse\nuB5g3rx55x8+fDguNctT6lq6Kc5Np7mzj3/95TZqW7p5wZIy5pXk8MtNNRxv7uLYqS7yMqO4O939\ng1y+qpIPvXw5++vbqSrOYdWcAtxdF9qJzAAz4fTRpEPBzF4C3AK80N1PjrdenT6aOZ441MTtG48R\nMSMzmsa3/3xo+IuD0tOMixaXsfloM6W5GXzg8mUUZqezq7aNJRV5zCrM4kBDB1eumaVTUSLTYCac\nPqoBqmNeV4XTnsbM1gJfB66cKBBkZnnegpKnfa3oa8+by47aVqqKsvnKgwfYV9/OlWtmseHwKW78\nwV9GXcc51UW8ZHkFv9t1gtyMKF9+y3mUhB3cg4POqc5eSvMyp6U9IhLfI4UoQUfzZQRh8ATwZnff\nHrPMPOD3wHUj+hfGpCOF5DMw6Dy4t4HOngFesKSUX20+zsHGTpbPyuNz9+6msb2XdfOL2VLTwryS\nHDKjEWqau5hXksO2mhauu2gBK2bls7e+ndevq6IyP0sjo0ROU8JPH4VFXAV8nmBI6jfd/VNmdgOA\nu99qZl8HXgcMdRL0T1S0QuHM0tbdx8n2XhaU5fKbrbV86Q/7yMlIIycjysYjp3j+4lLu23ECdzAL\nhtACrJlbwEtXVlKRn8W6BcUU52RQlpdBV98AORlR2nv6iUaM7r4BCrPT1a8hKW9GhEI8KBRSx+Cg\nE4kYpzp6Od7SRXleJvdur6O1u5/7dpxg89Hmpy2fnxWlrbuf6pJsjp3qGg6Q5ZX5XLqygvu21zG7\nMJv/+/LlnF1dNOZ2T7R2U5CVzvGWLjKjEeYWZStUJOkpFOSM19HTT0NbD08ePkVLVx97TrRRnp/J\ntpoW1swtJDMawcyGA2T1nAIa23to7+7npasq2XS0mfULSyjPz6R/0HndeVW0dffxtm88zoLSXA6f\n7KC7f5CsaIRLV1byxTeeo3CQpKVQEIlxsr2H4pwMGtp7+NBPN7Ovvp0Fpbk8dvAkETPMICs9jb6B\nQfKz0mlo66EwO503PK+avSfa+MPuBtYvLKG6JIfXnVfFytn5dPcNEk0z9p5oJzczjbVVRfQPDGJm\npI0YUdXdN8DjB5soyc1gzVzd7lymn0JBZBJOtHaTk5FGc2cf133zcRaV5fIfrz2LR/afZF5pDufN\nK2Zg0Ln+Oxs40NhBY1sPbT39o67rggUl7D7RRsTgpitWUJybwW0PH6KnfwCAjUeaSYsY9/zDxSyp\nyGP78VZqW7q5bEUFvQODuv+UxJVCQeQ0TeZCu+6+Ae7bcYL61m5yMqL0Dw5SkZ/J4wdP8eSRUyyr\nyGN/QzsbjwT9HfNKcujs7aexvZePXr2K/75/D7MLs3CHvfXtAFQWZAY3KVxewdqqIjYeOcWauQX8\nbmc9iyvyKMnJ4H2XLaU8P3PSdYqMpFAQSRB358G9jTR39nLVWbNp7uzjSFMn588v5rt/PsRn79nN\nmrmFvGLtbLr7Brh9Yw3nzy/mN9vqaGwPTlu1dPWxanYBrd191Lf2sLgij1eePZuBAecbDx/kmrPn\nMLc4m6z0NEpyM7h0RQU5Gc+87Gjz0Wbu3Hycq9fO5tx5+o7vVKZQEEkyvf2D1DR3Mbcom601LZw3\nrwgz4w+76nnfD/8yfNpqeWU+u0+0Pe29c4uyyYhGKMpJZ9Bh5ax8OnoH+NXm4wAsLMvl1+97IT/d\ncIwlFXm8YEnZ8Oiu7cdbyEiLsLQyf3h9Q58LZoa7446uPE9yCgWRM4i709E7QG1zF0sq8jjV2Ud6\nmtHTP8i2mhb+5/f7KM7JoL2nD8PYcqyZSMR4w7pqzptfzN99fyP5mdHhYJlblE1Dew9VRdkcOhl8\nIdMly8u5bGUlu2pbuXf7CapLsvn0a9fyT7/YSl1LN+9/2TJyM9JoaO/h6rVzuHtrLbdvPEbfgPPj\nd1846pGKzBwKBZEUNrLf4bt/PsTmYy28ZHkFR5o62XuijcKcdB4/2MTi8jyWVOTx7UcOcbKjl5yM\nNJ6/uJQHdjfQP+hkpUdYVJbHjtrWZ2xnSUUe++rbOXdeEcU5GTx/cSkXLirl4X2N1LV209TRy9KK\nPPoGnFesnc0Du+t5zblVlOdnsrO2lZvv28Orz53DVWtm09rdx9cfOsjbn79guP9kpEcPnGT1nALy\ns9Lj9as7YykUROS09A0M0tTRS2F2OlnpaTxxqIltNS28eFk5C0pzuW9HHR09A0TTjN11bVx11mxW\nzyng3+/eyXcfPczcouzh7+MAyEqPkJeZTmN7zzO2lZuRRjQtQlt3H4MenBIryknnsYNNPH9xKe+5\nZDF/2tfIW9fPp6o4m6aOXjYeaeZd39nAuvnFfOSqFSwpz6cw55nhsP14C+V5mVQUZLG7ro3i3HQq\n8rPi+rtLBgoFEZkWsX0OTxxqYntNC69YO4eyvAzcobW7j78cbeaHjx3hLRfOZ9ORZmpbuthR28pn\nr13L7ro2vvC7vRxo6ODipWU8tLdxeN1FOekMDDpt3f1EDMrzg5FaQx9beZlRFlfk0dXbT2tXP8tn\n5fPQ3gay09O4dGUld2+tpao4mzve+wLMjIf3NbKrtpXqkhxefe5cImZ85p5dHG3qZGDQyYhG+NRr\nzqKjp585RdkJ+o3Gh0JBRJLGwKCzq66VVbML2FHbSn1bDyU5GfzXb4MhvAvKcvnLkVPc+JKlZGek\ncbSpkx3f5tErAAAJ/0lEQVS1rTS09bCztpX8rHTyMtO4b8cJnreghJyMNB49cJK1VUU8sr+R9LQI\nPf2Dw7d2ByjNzaAwO50DjR3kZ0UZGHQ6ewfIyUijp3+QuUXZRNOM5ZX5HDrZyYcuX0bEjPmlOWw7\n3sqLlpZRlBPcmPGPexq4Z1sd6xeW8PONx/iXV6xi+ayg475vYJDHDjQxvzSH6pKchPx+QaEgIimo\nu29g+PYmQx4/2MTdW2vJy4xyyfJy1lYV8fD+Rn7+5DHae/p55do5XHPOHBy46Wdb2HSsmecvLqXm\nVBcHGjs43txFdnoard3PvGhxWWUea+YWcvvGp38rQHZ6GmvmFrD9eCtleZkcaeoE4D2XLGbH8VYy\nohHWLyyhqaOXWYVZXLaykvSI0dbTz3u/v5GllfksKsvl/PnFfP7+PVy2spLmzl4uW1nJhYtKn9Xv\nRqEgInKaYofiAnT1DtDS1ceAO1uONtPTP8iBhnYuWlzGhkNN/HJTDfsbOvjrFyzk5asr+fXWWt50\nwTy++aeDbDnWworZ+Rxo6OBtF87nrq21PLingfL8TLLT0zjS1EnEIObgBTMoyEpn0INTZkPThj6m\nP/CyZbzvsqXPqm0KBRGROOvtH6SupZt5pROfFuro6eenG47yqnPmUpyTTk1zF5UFWdSc6uK+HXVE\nIxH2nGjjbRfNZ9XsAk519vG1hw5w+apKnjx8ijVzC5/1UQIoFEREJMZkQyEyHcWIiEhyUCiIiMgw\nhYKIiAxTKIiIyDCFgoiIDFMoiIjIMIWCiIgMUyiIiMiwpLt4zcwagMPP8u1lQOOESyUHtWVmUltm\nJrUF5rt7+UQLJV0oPBdmtmEyV/QlA7VlZlJbZia1ZfJ0+khERIYpFEREZFiqhcJXE13AFFJbZia1\nZWZSWyYppfoURERkfKl2pCAiIuNQKIiIyLCUCQUzu8LMdpvZPjP7cKLrOV1mdsjMtprZJjPbEE4r\nMbPfmtne8GdxouscjZl908zqzWxbzLQxazezj4T7abeZvTwxVY9ujLZ83Mxqwn2zycyuipk3I9ti\nZtVm9gcz22Fm283sH8LpSbdfxmlLMu6XLDN73Mw2h235RDh9+vaLu5/xDyAN2A8sAjKAzcCqRNd1\nmm04BJSNmPZZ4MPh8w8Dn0l0nWPU/iLgPGDbRLUDq8L9kwksDPdbWqLbMEFbPg58aJRlZ2xbgNnA\neeHzfGBPWG/S7Zdx2pKM+8WAvPB5OvAYcOF07pdUOVK4ANjn7gfcvRf4EXBNgmuaCtcA3w6ffxt4\ndQJrGZO7Pwg0jZg8Vu3XAD9y9x53PwjsI9h/M8IYbRnLjG2Lu9e6+8bweRuwE5hLEu6Xcdoylpnc\nFnf39vBlevhwpnG/pEoozAWOxrw+xvj/aGYiB+43syfN7PpwWqW714bP64DKxJT2rIxVe7Luq783\nsy3h6aWhQ/ukaIuZLQDOJfirNKn3y4i2QBLuFzNLM7NNQD3wW3ef1v2SKqFwJnihu58DXAm818xe\nFDvTg2PJpBxfnMy1h/6X4NTkOUAtcHNiy5k8M8sDfg78H3dvjZ2XbPtllLYk5X5x94Hw/3oVcIGZ\nrRkxP677JVVCoQaojnldFU5LGu5eE/6sB35BcIh4wsxmA4Q/6xNX4Wkbq/ak21fufiL8jzwIfI2n\nDt9ndFvMLJ3gQ/T77n57ODkp98tobUnW/TLE3ZuBPwBXMI37JVVC4QlgqZktNLMM4I3AnQmuadLM\nLNfM8oeeA5cD2wja8PZwsbcDdySmwmdlrNrvBN5oZplmthBYCjyegPombeg/a+g1BPsGZnBbzMyA\nbwA73f2/YmYl3X4Zqy1Jul/KzawofJ4NvAzYxXTul0T3tk/XA7iKYFTCfuCfE13Pada+iGCEwWZg\n+1D9QCnwO2AvcD9Qkuhax6j/hwSH730E5zz/ZrzagX8O99Nu4MpE1z+JtnwX2ApsCf+Tzp7pbQFe\nSHAKYguwKXxclYz7ZZy2JON+WQv8Jax5G/DRcPq07Rfd5kJERIalyukjERGZBIWCiIgMUyiIiMgw\nhYKIiAxTKIiIyDCFgswYZvZI+HOBmb15itf9T6NtK17M7NVm9tE4rfufJl7qtNd5lpndNtXrleSj\nIaky45jZJQR3t7z6NN4Tdff+cea3u3veVNQ3yXoeAV7l7o3PcT3PaFe82mJm9wN/7e5Hpnrdkjx0\npCAzhpkN3R3y08DF4T3w3x/eIOxzZvZEeHOzd4fLX2JmD5nZncCOcNovw5sGbh+6caCZfRrIDtf3\n/dhtWeBzZrbNgu+reEPMuh8ws5+Z2S4z+3545Sxm9mkL7t2/xcz+c5R2LAN6hgLBzG4zs1vNbIOZ\n7TGzq8Ppk25XzLpHa8tbLbgH/yYz+4qZpQ210cw+ZcG9+R81s8pw+uvD9m42swdjVv8rgqv9JZUl\n+go+PfQYegDt4c9LgLtipl8P/Ev4PBPYQHDv+EuADmBhzLIl4c9sgitCS2PXPcq2Xgf8luA7NyqB\nIwT3578EaCG4l0wE+DPBlbOlBFeODh1lF43SjncCN8e8vg24J1zPUoIrobNOp12j1R4+X0nwYZ4e\nvr4FuC587sArw+efjdnWVmDuyPqBFwC/SvS/Az0S+4hONjxEEuhyYK2ZXRu+LiT4cO0FHvfgPvJD\n3mdmrwmfV4fLnRxn3S8EfujuAwQ3Hfsj8DygNVz3MQALbmW8AHgU6Aa+YWZ3AXeNss7ZQMOIaT/x\n4MZse83sALDiNNs1lsuA84EnwgOZbJ66WVpvTH1PEtxHB+Bh4DYz+wlw+1Oroh6YM4ltyhlMoSDJ\nwIC/d/d7nzYx6HvoGPH6pcBF7t5pZg8Q/EX+bPXEPB8Aou7eb2YXEHwYXwvcCFw64n1dBB/wsUZ2\n3jmTbNcEDPi2u39klHl97j603QHC/+/ufoOZrQdeATxpZue7+0mC31XXJLcrZyj1KchM1EbwtYpD\n7gXeY8HtkTGzZeHdYkcqBE6FgbCC4GsMh/QNvX+Eh4A3hOf3ywm+bnPMu0xacM/+Qne/G3g/cPYo\ni+0EloyY9nozi5jZYoIbHO4+jXaNFNuW3wHXmllFuI4SM5s/3pvNbLG7P+buHyU4ohm69fIynrqT\nqKQoHSnITLQFGDCzzQTn479AcOpmY9jZ28DoXz16D3CDme0k+NB9NGbeV4EtZrbR3d8SM/0XwEUE\nd6B14B/dvS4MldHkA3eYWRbBX+kfGGWZB4Gbzcxi/lI/QhA2BcAN7t5tZl+fZLtGelpbzOxfgPvM\nLEJw99b3AofHef/nzGxpWP/vwrYDvAT49SS2L2cwDUkViQMz+wJBp+394fj/u9z9Zwkua0xmlgn8\nkeAb/sYc2itnPp0+EomPfwdyEl3EaZgHfFiBIDpSEBGRYTpSEBGRYQoFEREZplAQEZFhCgURERmm\nUBARkWH/H/3hq2TyCEsPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12c403be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.967299\n",
      "Test Accuracy: 0.944217\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "dataFile = '/Users/zhangdan/Documents/GitHub/Volve-Fault-Detection/DNN_NEW/saveddata.mat'\n",
    "data = scio.loadmat(dataFile)\n",
    "Y_test=data['Y_test']\n",
    "Y_train=data['Y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 35)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Anomaly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file='/Users/zhangdan/Documents/GitHub/Volve-Fault-Detection/DNN_NEW/Anomaly.mat'\n",
    "data=scio.loadmat(file)\n",
    "Anomaly=data['Anomaly']\n",
    "Anomaly_Y=data['Anomaly_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=np.ones((1,n))\n",
    "for i in range(n):\n",
    "    result[0,i]=predict(Anomaly[:,i].reshape((1, 280)).T, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[m,n]=Anomaly.shape\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "\n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "\n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "\n",
    "    x = tf.placeholder(\"float\", [280, 1])\n",
    "\n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.argmax(z3)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "my_image = \"thumbs_up.jpg\"\n",
    "## END CODE HERE ##\n",
    "\n",
    "# We preprocess your image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\n",
    "my_image_prediction = predict(my_image, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"Your algorithm predicts: y = \" + str(np.squeeze(my_image_prediction)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
